
\chapter{Problem}
\label{cha:Problem}

Current systems for distributed computing fall victim to certain issues, especially when considering the amount of data transfer that is required for these systems to function. Namely, these are are a greater amount of shared state, vulnerability of information when sent through insecure channels, increased network flow, and increased computation on cloud devices. The first represents a problem for programmers, the second represents a security risk, and the last two directly increase the cost of cloud computing services.

It is known that shared mutable state is undesirable in most applications \cite{alma991003995329707681}. Shared mutable state leads to unpredictable behavior regarding the state of a system, making debugging a harder task and adding unnecesary complexity when trying to understand and reason about programs. This is also true in concurrent and parallel applications, and by extension distributed systems. By trying to apply this principle to distributed systems we see that \textit{pass-by-value} and \textit{pass-by-reference} are problematic, since they imply the duplication of data in the former and shared mutable state in the latter. 

Another problem of distributed computing is the susceptibility of data when in transit. We can usually consider each individual node as safe, but the channels through which they communicate may not be. This is especially the case when our nodes communicate via the internet. Man in the middle attacks are just one example of a way in which an attacker can gain access to information being sent through the network. Data encryption gives us a way to protect our information from prying eyes, but there are ways attackers can find holes in this type of armour. A prudent approach could be to avoid unnecesary data transfer whenever possible to avoid the possibility of falling victim to an attack of this kind.

A large amount of network flow is an evident issue with distributed systems that seems almost unavoidable. Clearly there will come a point where data needs to travel between two nodes, this is a defining feature of distributed systems. In our modern world of big data, sending gigabytes of data is commonplace, even the transfer of terabytes of data is an everyday ocurrance in certain businesses and industries. The only way to avoid this is to find ways to reduce the size of the data being sent, either by lossless data compression algorithms or by performing some kind of preprocessing. 
% i think i can add more but idk

When considering a cloud computing infrastructure, architects are forced to make decisions of which services will live on which machines. Even in the best designed applications, there will always be some kind of wasted computation that could have been done on cheaper machines. This can be mitigated by using a serverless architecture, but these also come with their own drawbacks that must be considered. It would be ideal to be able to choose where a computation will be performed based on the expected complexity.

These last two issues are especially relevant when using resources in the cloud since they directly increases in expenses. By reducing amount of data sent and received between nodes, and moving around computation to devices that would give the best cost to execution time rate, we can expect reasonable savings on cloud service spending. It would also be attractive for our solution to address the first two problems we highlighted, since this would improve the maintainability and security of our solution.


